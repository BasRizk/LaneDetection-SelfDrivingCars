{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.5.5 64-bit ('sdc_env': conda)",
      "language": "python",
      "name": "python35564bitsdcenvconda2fbd552390024937bca62d20770ab0c4"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5-final"
    },
    "colab": {
      "name": "Project_1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "82uA9wzmJav4"
      },
      "outputs": [],
      "source": [
        "# Project Steps\n",
        " ## Main Steps:\n",
        "\n",
        "* Camera calibration and distortion correction.\n",
        "* Color/gradient threshold.\n",
        "* Perspective transform.\n",
        "* Detect lane lines.\n",
        "\n",
        "## Extra Step:\n",
        "* Determine the lane curvature."
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "CzLFGoFcdN2-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy.polynomial.polynomial as poly\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "test_dir = \"test_images\"\n",
        "test_files = os.listdir(test_dir)\n",
        "test_files"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "zYIX_WJ5Jav5"
      },
      "outputs": [],
      "source": [
        "# Camera Calibration  and Distortion Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "KAVcKmXveh6_"
      },
      "outputs": [],
      "source": [
        "\n",
        "*   Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cameraCalibrate():\n",
        "    # Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
        "    import glob\n",
        "\n",
        "    #read in and make a list of calibration images\n",
        "    images = glob.glob('./camera_cal/calibration*.jpg')\n",
        "\n",
        "    #Arrays to store objects points and image points from all the images\n",
        "    objpoints = [] #3d points\n",
        "    imgpoints = [] #2d points\n",
        "\n",
        "    #prepare points\n",
        "    objp  = np.zeros((9*6,3),np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
        "\n",
        "\n",
        "    for fname in images:\n",
        "        #read in each image\n",
        "        img = mpimg.imread(fname)\n",
        "        #convert image to gray scale  \n",
        "        gray  = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
        "        #find corners  \n",
        "        ret,corners = cv2.findChessboardCorners(gray,(9,6),None)\n",
        "        if ret == True:\n",
        "            imgpoints.append(corners)\n",
        "            objpoints.append(objp)\n",
        "            \n",
        "    shape =(img.shape[1],img.shape[0])\n",
        "    ret, mtx, dist, rvect, tvect = cv2.calibrateCamera(objpoints,imgpoints,shape,None,None)\n",
        "    \n",
        "    #undistored the image\n",
        "    # undistorted_image = cv2.undistort(image, mtx, dist, None, mtx)\n",
        "    return mtx,dist,mtx"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "camCalibration = cameraCalibrate()\n",
        "\n",
        "def undistort_image(image, verbose=0):\n",
        "    output = cv2.undistort(image, camCalibration[0], camCalibration[1], None, camCalibration[2])\n",
        "    if (verbose > 0):\n",
        "        f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "        ax1.set_title('Original image', fontsize=20)\n",
        "        ax1.imshow(image)\n",
        "        ax2.set_title('calibrated image', fontsize=20)\n",
        "        ax2.imshow(output)\n",
        "    return output\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "NgpLeFaRe9s2"
      },
      "outputs": [],
      "source": [
        "* Apply (Calibrate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = 'test5.jpg'\n",
        "sample_image = mpimg.imread(test_dir + \"/\" + filename)\n",
        "# sample_image = mpimg.imread('./camera_cal/calibration1.jpg')\n",
        "\n",
        "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax1.set_title('Original image', fontsize=20)\n",
        "ax1.imshow(sample_image)\n",
        "ax2.set_title('calibrated image', fontsize=20)\n",
        "calibrated_image=undistort_image(sample_image)\n",
        "ax2.imshow(undistort_image(sample_image))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "PULmYxJQJav-"
      },
      "outputs": [],
      "source": [
        "# Gradient / Color Threshold + Sobel"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize(ch1, ch2, ch3,\n",
        "            ch1_name, ch2_name, ch3_name,\n",
        "            plot):\n",
        "    plot[0].set_title(ch1_name + '_channel', fontsize=20)\n",
        "    plot[0].imshow(ch1,cmap='gray')\n",
        "    plot[1].set_title(ch2_name + '_channel', fontsize=20)\n",
        "    plot[1].imshow(ch2,cmap='gray')\n",
        "    plot[2].set_title(ch3_name + '_channel', fontsize=20)\n",
        "    plot[2].imshow(ch3,cmap='gray')\n",
        "\n",
        "def binarize(array, l_thresh, u_thresh, yes=1, no=0):\n",
        "    binary = (array >= l_thresh) * array\n",
        "    binary = (array <= u_thresh) * binary\n",
        "    binary = (binary > 0)*yes\n",
        "    return binary\n",
        "\n",
        "# Color threshold for the white line\n",
        "def white(img, channel_num = 0, white_thresh = (220,255)):\n",
        "    channel = img[:,:,channel_num]    \n",
        "    l_thresh, u_thresh = white_thresh[:]\n",
        "    binary = binarize(channel, l_thresh, u_thresh, yes=255, no=0)\n",
        "    return binary\n",
        "\n",
        "# Color threshold for the yellow line\n",
        "def yellow(img, channel_num = 2, yellow_thresh = (220,255)):\n",
        "    channel = img[:,:,channel_num]\n",
        "    l_thresh, u_thresh = yellow_thresh[:]\n",
        "    # binary = cv2.inRange(img, np.array([220, 220, 220]), np.array([255, 255, 255]))\n",
        "    binary = binarize(channel, l_thresh, u_thresh, yes=255, no=0)\n",
        "    return binary"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gHn67GNZJav_"
      },
      "outputs": [],
      "source": [
        "* Possible Models\n",
        "\n",
        "-- HLS Model\n",
        "\n",
        "-- HSV Model\n",
        "\n",
        "-- LAB Model\n",
        "\n",
        "-- RGB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_in_action = np.copy(calibrated_image)\n",
        "f, plot_axs = plt.subplots(4, 3, figsize=(20,20))\n",
        "\n",
        "hls_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2HLS)\n",
        "h_channel = hls_image[:,:,0]\n",
        "l_channel = hls_image[:,:,1]\n",
        "s_channel = hls_image[:,:,2]\n",
        "visualize(h_channel, l_channel, s_channel, \"h\", \"l\", \"s\", plot_axs[0])\n",
        "\n",
        "hsv_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2HSV)\n",
        "h_channel = hsv_image[:,:,0]\n",
        "s_channel = hsv_image[:,:,1]\n",
        "v_channel = hsv_image[:,:,2]\n",
        "visualize(h_channel, s_channel, v_channel, \"h\", \"s\", \"v\", plot_axs[1])\n",
        "\n",
        "lab_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2LAB)\n",
        "l_channel = lab_image[:,:,0]\n",
        "a_channel = lab_image[:,:,1]\n",
        "b_channel = lab_image[:,:,2]\n",
        "visualize(l_channel, a_channel, b_channel, \"l\", \"a\", \"b\", plot_axs[2])\n",
        "\n",
        "rgb_image = image_in_action\n",
        "r_channel = rgb_image[:,:,0]\n",
        "g_channel = rgb_image[:,:,1]\n",
        "b_channel = rgb_image[:,:,2]\n",
        "visualize(r_channel, g_channel, b_channel, \"r\", \"g\", \"b\", plot_axs[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Sobel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X or Y sobel gradient\n",
        "def abs_sobel_thresh(image, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
        "    # TODO: Convert to grayscale using cv2.COLOR_RGB2GRAY as the conversion\n",
        "    if len(image.shape) == 2:\n",
        "        gray_img = image\n",
        "    else:\n",
        "        gray_img = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # TODO: Apply x or y gradient with the OpenCV Sobel() function\n",
        "    # and take the absolute value    \n",
        "    if orient == 'x':\n",
        "        abs_sobel = np.abs(cv2.Sobel(gray_img,cv2.CV_64F,1,0,ksize=sobel_kernel))\n",
        "    else:\n",
        "        abs_sobel = np.abs(cv2.Sobel(gray_img,cv2.CV_64F,0,1,ksize=sobel_kernel))\n",
        "   \n",
        "    # TODO: Rescale back to 8 bit integer\n",
        "    abs_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
        "    \n",
        "    # TODO: Create a binary image of ones where threshold is met, zeros otherwise\n",
        "#     abs_sobel_output = np.zeros_like(abs_sobel)\n",
        "#     abs_sobel_output = abs_sobel[abs_sobel > thresh[0] or abs_sobel < thresh[1]].astype(int)\n",
        "    abs_sobel_output = binarize(abs_sobel, thresh[0], thresh[1], yes=255)\n",
        "    # Return the binary image\n",
        "    return abs_sobel_output"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Method (USING SOBEL OVER COLOR MODELS, USING COLOR THRESHOLDING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_binary_thresholded_img(calibrated_image, verbose=0, gaussian_kernel = (0,0),\n",
        "#                                 yellow_thresh=(170,200), white_thresh=(220,255), sobel_thresh=(40,55), sobel_kernel=5):\n",
        "#     image_in_action = calibrated_image.copy()\n",
        "#     image_in_action = cv2.GaussianBlur(image_in_action, (0,0), cv2.BORDER_DEFAULT)\n",
        "\n",
        "#     lab_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2LAB)\n",
        "#     hsv_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2HSV)\n",
        "\n",
        "#     # binary_white = white(hsv_image,  white_thresh=(220,255),channel_num=2)\n",
        "#     # binary_yellow = yellow(lab_image, yellow_thresh=(170,200),channel_num=2)\n",
        "#     # x_sobel = abs_sobel_thresh(image_in_action, orient='x', sobel_kernel=5, thresh=(45, 55))\n",
        "#     # y_sobel = abs_sobel_thresh(image_in_action, orient='y', sobel_kernel=5, thresh=(45, 55))\n",
        "#     # combine both the white, yellow and x_sobel binaries in combined_binary\n",
        "\n",
        "#     binary_white_x = abs_sobel_thresh(hsv_image[:,:,2], orient='x', sobel_kernel=sobel_kernel, thresh=sobel_thresh)\n",
        "#     binary_yellow_x = abs_sobel_thresh(lab_image[:,:,2], orient='x', sobel_kernel=sobel_kernel, thresh=sobel_thresh)\n",
        "#     binary_white_y = abs_sobel_thresh(hsv_image[:,:,2], orient='y', sobel_kernel=sobel_kernel, thresh=sobel_thresh)\n",
        "#     binary_yellow_y = abs_sobel_thresh(lab_image[:,:,2], orient='y', sobel_kernel=sobel_kernel, thresh=sobel_thresh)\n",
        "\n",
        "#     binary_combined = np.zeros_like(image_in_action)\n",
        "#     # binary_combined[(binary_white >= 1) | (binary_yellow >= 1) | (x_sobel >= 1)|(y_sobel >= 1)] = 255\n",
        "#     binary_combined[(binary_white_x >= 1) | (binary_yellow_x >= 1) | (binary_white_y >= 1) | (binary_yellow_y >= 1)] = 255\n",
        "#     if verbose > 0:\n",
        "#         # visualize\n",
        "#         f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,10))\n",
        "#         ax1.set_title('binary white x', fontsize=20)\n",
        "#         ax1.imshow(binary_white_x, cmap='gray')\n",
        "#         ax2.set_title('binary white y', fontsize=20)\n",
        "#         ax2.imshow(binary_white_y, cmap='gray')\n",
        "#         ax3.set_title('binary yellow y', fontsize=20)\n",
        "#         ax3.imshow(binary_yellow_y, cmap='gray')\n",
        "#         ax4.set_title('binary yellow x', fontsize=20)\n",
        "#         ax4.imshow(binary_yellow_x, cmap='gray')\n",
        "\n",
        "#         f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "#         ax1.set_title('image_of_application', fontsize=20)\n",
        "#         ax1.imshow(image_in_action, cmap='gray')\n",
        "#         ax2.set_title('combined', fontsize=20)\n",
        "#         ax2.imshow(binary_combined, cmap='gray')\n",
        "#     return binary_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_binary_thresholded_img(calibrated_image, verbose=0, gaussian_kernel = (0,0),\n",
        "                                yellow_thresh=(170,200), white_thresh=(220,255), sobel_thresh=(40,55), sobel_kernel=5):\n",
        "    img = calibrated_image.copy()\n",
        "    image_in_action = cv2.GaussianBlur(img, gaussian_kernel, cv2.BORDER_DEFAULT)\n",
        "\n",
        "    lab_image = cv2.cvtColor(img,cv2.COLOR_RGB2LAB)\n",
        "    hsv_image = cv2.cvtColor(img,cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    binary_white = white(hsv_image,  white_thresh=white_thresh,channel_num=2)\n",
        "    binary_yellow = yellow(lab_image, yellow_thresh=yellow_thresh,channel_num=2)\n",
        "    x_sobel = abs_sobel_thresh(img, orient='x', sobel_kernel=sobel_kernel, thresh=sobel_thresh)\n",
        "    # y_sobel = abs_sobel_thresh(img, orient='y', sobel_kernel=sobel_kernel, thresh=sobel_thresh)\n",
        "\n",
        "    # combine both the white, yellow and x_sobel binaries in combined_binary\n",
        "    binary_combined = np.zeros_like(image_in_action)\n",
        "    # binary_combined[(binary_white >= 1) | (binary_yellow >= 1) | (x_sobel >= 1) | (y_sobel >= 1)] = 255\n",
        "    binary_combined[(binary_white >= 1) | (binary_yellow >= 1) | (x_sobel >= 1)] = 255\n",
        "\n",
        "    if verbose > 0:\n",
        "        # visualize\n",
        "        f, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20,10))\n",
        "        ax1.set_title('binary white', fontsize=20)\n",
        "        ax1.imshow(binary_white, cmap='gray')\n",
        "        ax2.set_title('binary yellow', fontsize=20)\n",
        "        ax2.imshow(binary_yellow, cmap='gray')\n",
        "        ax3.set_title('x_sobel', fontsize=20)\n",
        "        ax3.imshow(x_sobel, cmap='gray')\n",
        "        # ax4.set_title('y_sobel', fontsize=20)\n",
        "        # ax4.imshow(y_sobel, cmap='gray')\n",
        "        f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "        ax1.set_title('blurred image', fontsize=20)\n",
        "        ax1.imshow(image_in_action, cmap='gray')\n",
        "        ax2.set_title('combined', fontsize=20)\n",
        "        ax2.imshow(binary_combined, cmap='gray')\n",
        "\n",
        "    return binary_combined"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "image_in_action = calibrated_image.copy()\n",
        "image_in_action = cv2.GaussianBlur(image_in_action, (0,0), cv2.BORDER_DEFAULT)\n",
        "\n",
        "lab_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2LAB)\n",
        "hsv_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2HSV)\n",
        "\n",
        "# binary_white = white(hsv_image,  white_thresh=(220,255),channel_num=2)\n",
        "# binary_yellow = yellow(lab_image, yellow_thresh=(170,200),channel_num=2)\n",
        "# x_sobel = abs_sobel_thresh(image_in_action, orient='x', sobel_kernel=5, thresh=(45, 55))\n",
        "# y_sobel = abs_sobel_thresh(image_in_action, orient='y', sobel_kernel=5, thresh=(45, 55))\n",
        "# combine both the white, yellow and x_sobel binaries in combined_binary\n",
        "\n",
        "binary_white_x = abs_sobel_thresh(hsv_image[:,:,2], orient='x', sobel_kernel=5, thresh=(45, 55))\n",
        "binary_yellow_x = abs_sobel_thresh(lab_image[:,:,2], orient='x', sobel_kernel=5, thresh=(45, 55))\n",
        "binary_white_y = abs_sobel_thresh(hsv_image[:,:,2], orient='y', sobel_kernel=5, thresh=(45, 55))\n",
        "binary_yellow_y = abs_sobel_thresh(lab_image[:,:,2], orient='y', sobel_kernel=5, thresh=(45, 55))\n",
        "\n",
        "binary_combined = np.zeros_like(image_in_action)\n",
        "# binary_combined[(binary_white >= 1) | (binary_yellow >= 1) | (x_sobel >= 1)|(y_sobel >= 1)] = 255\n",
        "binary_combined[(binary_white_x >= 1) | (binary_yellow_x >= 1) | (binary_white_y >= 1) | (binary_yellow_y >= 1)] = 255\n",
        "\n",
        "# visualize\n",
        "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,10))\n",
        "ax1.set_title('binary white x', fontsize=20)\n",
        "ax1.imshow(binary_white_x, cmap='gray')\n",
        "ax2.set_title('binary white y', fontsize=20)\n",
        "ax2.imshow(binary_white_y, cmap='gray')\n",
        "ax3.set_title('binary yellow y', fontsize=20)\n",
        "ax3.imshow(binary_yellow_y, cmap='gray')\n",
        "ax4.set_title('binary yellow x', fontsize=20)\n",
        "ax4.imshow(binary_yellow_x, cmap='gray')\n",
        "\n",
        "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax1.set_title('image_of_application', fontsize=20)\n",
        "ax1.imshow(image_in_action, cmap='gray')\n",
        "ax2.set_title('combined', fontsize=20)\n",
        "ax2.imshow(binary_combined, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "o4k2Dpf2JawE"
      },
      "outputs": [],
      "source": [
        "# Perspective Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Utils (Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perspective_transform(undistorted, inverse=False, verbose=0):   \n",
        "\n",
        "    img = np.copy(undistorted)\n",
        "    w = img.shape[1]\n",
        "    h = img.shape[0]\n",
        "    img_size = (w, h)\n",
        "    src = np.float32([[0,h - 2],[3.6*w/8,3.7*h/6],\n",
        "                  [4.4*w/8,3.7*h/6],[w,h - 2]])\n",
        "    # make sure that the points follow the right arrangement whether it's clockwise or counter-clockwise\n",
        "    # source and destination points must have the same arrangement whether it's clockwise or counter-clockwise\n",
        "    # The points in src array are (x,y).\n",
        "    dst = np.float32([[2*w/10,h -2],[2*w/10,2*h/10],\n",
        "                    [8*w/10,2*h/10],[8*w/10,h - 2]])\n",
        "\n",
        "    if(inverse):\n",
        "        M = cv2.getPerspectiveTransform(dst, src)\n",
        "    else:\n",
        "        M = cv2.getPerspectiveTransform(src, dst)\n",
        "\n",
        "    transformed_img = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # gray_scale = False\n",
        "\n",
        "    # if len(img.shape) == 2:\n",
        "    #     print(img.shape)\n",
        "    #     gray_scale = True\n",
        "    #     img = cv2.cvtColor(undistorted, cv2.COLOR_GRAY2RGB)\n",
        "    # M = cv2.getPerspectiveTransform(src, dst)\n",
        "    # transformed_img = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
        "\n",
        "\n",
        "    # if gray_scale:\n",
        "    #     transformed_img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # visualize your mask region\n",
        "    if(verbose > 0):\n",
        "        f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "        ax1.set_title('source area', fontsize=20)\n",
        "        ax1.imshow(undistorted)\n",
        "        ordered_y = [src[0][1],src[1][1],src[2][1],src[3][1],src[0][1]]\n",
        "        ordered_x = [src[0][0],src[1][0],src[2][0],src[3][0],src[0][0]]\n",
        "        ax1.plot(ordered_x,ordered_y , color='red', alpha=0.7,\n",
        "            linewidth=3, solid_capstyle='round', zorder=2)\n",
        "\n",
        "        # visualize your mask region\n",
        "        f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "        ax1.set_title('destination area', fontsize=20)\n",
        "        ax1.imshow(undistorted)\n",
        "        ordered_y = [dst[0][1],dst[1][1],dst[2][1],dst[3][1],dst[0][1]]\n",
        "        ordered_x = [dst[0][0],dst[1][0],dst[2][0],dst[3][0],dst[0][0]]\n",
        "        ax1.plot(ordered_x,ordered_y , color='red', alpha=0.7,\n",
        "            linewidth=3, solid_capstyle='round', zorder=2)\n",
        "\n",
        "        # visualize your results\n",
        "        f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "        ax1.set_title('Original image', fontsize=20)\n",
        "        ax1.imshow(undistorted)\n",
        "        ax2.set_title('Transformed image', fontsize=20)\n",
        "        ax2.imshow(transformed_img)\n",
        "\n",
        "    return transformed_img"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply (Transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Undistord the image then apply perspective transformation\n",
        "transformed_image = perspective_transform(calibrated_image, verbose=1)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "VzZTfbdgJawH"
      },
      "outputs": [],
      "source": [
        "# Detect Lane Lines\n",
        "\n",
        "After finishing the previous steps You now have a thresholded warped image and you're ready to map out the lane lines! There are many ways you could go about this, but here's one example of how you might do it:\n",
        "### Peaks in a Histogram and Sliding Windows\n",
        "* After applying calibration, thresholding, and a perspective transform to a road image, you should have a binary image where the lane lines stand out clearly. However, you still need to decide explicitly which pixels are part of the lines and which belong to the left line and which belong to the right line.\n",
        "* we can use the two highest peaks from our histogram as a starting point for determining where the lane lines are, and then use sliding windows moving upward in the image (further along the road) to determine where the lane lines go.\n",
        "#### steps:\n",
        "  1. split the histogram into two sides, one for each lane line.\n",
        "  2. Set up sliding windows and window hyperparameters:\n",
        "     * set a few hyperparameters related to our sliding windows, and set them up to iterate across the binary activations in the image. These hyperparameters are:\n",
        "        1. **W_Number**; number of sliding windows.\n",
        "        2. **Margin**; the width of each window.\n",
        "        3. **Minimum_pixels**; used as a threshold to recenter the next sliding window.\n",
        "        4. **Window_Height**; computed from number of pixels and image height.\n",
        "  3. Loop through each window in W_Number.\n",
        "  4. Find the boundaries of our current window. This is based on a combination of the current window's starting point      , as well as the margin you set in the hyperparameters.\n",
        "  5. Use cv2.rectangle to draw these window boundaries onto visualization image.\n",
        "  6. Now that we know the boundaries of our window, find out which activated(non zero) pixels actually fall into the window.\n",
        "  7. Append these non zero pixels to two different arrays one for the right line and the other for the left line.\n",
        "  8. If the number of pixels you found in Step **6** are greater than your hyperparameter Minimum_pixels, re-center our window based on the mean position of these pixels.\n",
        "  9. Now that we have found all our pixels belonging to each line through the sliding window method, it's time to fit a polynomial to the line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Sliding Window (Method)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detectlanes(w_number, margin, minimum_pixels, binary_img, left_peak=-1, right_peak=-1, verbose=0):\n",
        "\n",
        "    out_img = binary_img.copy()\n",
        "    # get the image's width and height.\n",
        "    w = out_img.shape[1]\n",
        "    h = out_img.shape[0]\n",
        "\n",
        "    if(left_peak < 0 or right_peak < 0):\n",
        "        left_peak = w//4\n",
        "        right_peak = w * 3 //4\n",
        "\n",
        "    # 2. Set up sliding windows and window hyperparameters:\n",
        "    window_height = int(h // w_number)\n",
        "\n",
        "    half_window_height = int(window_height // 2)\n",
        "    half_window_width = int(margin // 1.5)\n",
        "    current_height = int(h - half_window_height)\n",
        "\n",
        "    left_window_center = int(left_peak)\n",
        "    right_window_center = int(right_peak)\n",
        "    left_fit = np.int32(np.empty((0,2)))\n",
        "    right_fit = np.int32(np.empty((0,2)))\n",
        "    \n",
        "    \n",
        "    # 3. Loop through each window in W_Number.\n",
        "    for i in range(0, w_number):\n",
        "        if(i > 0):\n",
        "            half_window_width = int(margin // 2)\n",
        "        left_window_boundaries = [( left_window_center - half_window_width, current_height + half_window_height) , (left_window_center + half_window_width, current_height - half_window_height)]\n",
        "        right_window_boundaries = [(right_window_center - half_window_width, current_height + half_window_height) , (right_window_center + half_window_width, current_height - half_window_height)]\n",
        "        # print(left_window_boundaries[0])\n",
        "        # print(left_window_boundaries[1])\n",
        "        \n",
        "        # print(left_window_boundaries[0])\n",
        "        # left_window = out_img[left_window_boundaries[1][1]: left_window_boundaries[0][1],left_window_boundaries[0][0]:left_window_boundaries[1][0],:]\n",
        "        # right_window = out_img[right_window_boundaries[1][1]: right_window_boundaries[0][1],right_window_boundaries[0][0]:right_window_boundaries[1][0],:]\n",
        "        left_window_indices, left_horizontal_indcies = window_helper(left_window_boundaries[1][1],left_window_boundaries[0][1],left_window_boundaries[0][0],left_window_boundaries[1][0], out_img)\n",
        "\n",
        "\n",
        "        right_window_indices, right_horizontal_indcies = window_helper(right_window_boundaries[1][1],right_window_boundaries[0][1],right_window_boundaries[0][0],right_window_boundaries[1][0], out_img)\n",
        "\n",
        "        # print(current_height)\n",
        "        # print(right_window_center)\n",
        "        if(len(left_horizontal_indcies) > minimum_pixels):\n",
        "            left_window_center = int(np.mean(left_horizontal_indcies))\n",
        "            left_window_boundaries = [( left_window_center - half_window_width, current_height + half_window_height) , (left_window_center + half_window_width, current_height - half_window_height)]\n",
        "\n",
        "        if(len(right_horizontal_indcies) > minimum_pixels):\n",
        "            right_window_center = int(np.mean(right_horizontal_indcies))\n",
        "            right_window_boundaries = [(right_window_center - half_window_width, current_height + half_window_height) , (right_window_center + half_window_width, current_height - half_window_height)]\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        if(current_height > 0):\n",
        "            left_fit = np.append(left_fit, [(left_window_center,current_height)], axis=0)\n",
        "            right_fit = np.append(right_fit,[(right_window_center,current_height)] , axis =0)\n",
        "\n",
        "        # if(i == 0):\n",
        "        #     left_fit = left_window_indices\n",
        "        #     right_fit = right_window_indices\n",
        "        # else:\n",
        "        #     left_fit = np.append(left_fit, left_window_indices, axis=0)\n",
        "        #     right_fit = np.append(right_fit, right_window_indices , axis =0)\n",
        "\n",
        "        for index in left_fit:\n",
        "            # print(index)\n",
        "            cv2.circle(out_img, tuple(index), 3, (255, 0, 0) , -1) \n",
        "\n",
        "        for index in right_fit:\n",
        "            cv2.circle(out_img, tuple(index), 3, (255, 0, 0) , -1)    \n",
        "        # print(left_window_boundaries[0])\n",
        "        cv2.rectangle(out_img, left_window_boundaries[0], left_window_boundaries[1], (0, 255, 0), 2)\n",
        "        cv2.rectangle(out_img, right_window_boundaries[0], right_window_boundaries[1], (0, 255, 0), 2)\n",
        "        cv2.circle(out_img, (left_window_center,current_height), 2, (255, 0, 0) , -1) \n",
        "        cv2.circle(out_img, (right_window_center,current_height), 2, (255, 0, 0) , -1) \n",
        " \n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "        current_height -= (window_height + 4)\n",
        "\n",
        "    if(verbose > 0):\n",
        "        f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "        ax1.set_title('Sliding Windows', fontsize=20)\n",
        "\n",
        "        ax1.imshow(out_img)\n",
        "    # print(left_fit.shape)\n",
        "    return left_fit, right_fit\n",
        "\n",
        "def window_helper(min_height, max_height, min_width, max_width, img):\n",
        "    test = img[min_height:max_height,min_width:max_width,:]\n",
        "    # print(img_copy.shape)\n",
        "    # print(test.shape)\n",
        "    x,y,z = np.where(test == (255,255,255))\n",
        "    x += min_height\n",
        "    y += min_width\n",
        "    # print(x)\n",
        "    # print(y)\n",
        "    # mean = 0\n",
        "    # if(len(x) > min_pixels):\n",
        "    #     mean = (int(np.mean(x)), int(np.mean(y)))\n",
        "    arr = np.array((y,x))\n",
        "\n",
        "    # print(arr[0])\n",
        "    # print(arr[1])\n",
        "    # final = np.reshape(np.ravel(arr),(-1,2),order='F')\n",
        "    return arr, y"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Histogram Peaks (Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# binary_white = white(calibrated_image, channel_num=0, white_thresh = (245,255))\n",
        "# binary_yellow = yellow(lab_image, channel_num=2, yellow_thresh = (170,200))\n",
        "\n",
        "# # combine both the white and yellow binaries in combined_binary\n",
        "# binary_combined = np.zeros_like(image_in_action)\n",
        "# binary_combined[(binary_white >= 1) | (binary_yellow >= 1)] = 255\n",
        "\n",
        "def get_histogram_peaks(binary_transformed_image, percentage=1, verbose=0):\n",
        "    # print(binary_transformed_image.shape)\n",
        "    image_histo = np.zeros_like(binary_transformed_image)\n",
        "    image_histo[binary_transformed_image >= 1] = 1 \n",
        "    # print(image_histo[:,:,1])\n",
        "    image_histo = image_histo[:,:,1]\n",
        "    row_start_index = int(image_histo.shape[0]*(1-percentage))\n",
        "    image_histo = image_histo[row_start_index:]\n",
        "    image_histo = image_histo.reshape(image_histo.shape[1], image_histo.shape[0])\n",
        "    image_histo = image_histo.sum(axis=1)\n",
        "    # print(image_histo)\n",
        "\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "    ax1.set_title('Transformed Image', fontsize=20)\n",
        "    ax1.imshow(binary_transformed_image)\n",
        "    ax2.set_title('Histogram', fontsize=20)\n",
        "    # x_values = np.array([i for i in range(0, binary_transformed_image.shape[1])])\n",
        "    # print(binary_transformed_image.shape)\n",
        "    x_new = np.linspace(0, binary_transformed_image.shape[1], num=binary_transformed_image.shape[1])\n",
        "\n",
        "    ax2.plot(x_new, image_histo)\n",
        "\n",
        "    image_center = int(image_histo.shape[0]/2)\n",
        "    left_peak = np.argmax(image_histo[:image_center])\n",
        "    right_peak = np.argmax(image_histo[image_center:]) + image_center\n",
        "    print(\"Left peak %s, Right peak %s\" % (left_peak, right_peak))\n",
        "    return left_peak, right_peak\n",
        "# print(left_fit.shape)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply (Using histogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformed_image = perspective_transform(calibrated_image)\n",
        "binary_transformed_image = get_binary_thresholded_img(transformed_image, white_thresh=(200,255), yellow_thresh=(140,200),verbose=1)\n",
        "left_peak,right_peak = get_histogram_peaks(binary_transformed_image, percentage=0.2, verbose=1)\n",
        "left_fit, right_fit = detectlanes(20,200,10,binary_transformed_image, left_peak=left_peak, right_peak=right_peak, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply (Not using histogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "left_fit, right_fit = detectlanes(20,200,10,binary_transformed_image, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "NaUKAAhqJawL"
      },
      "outputs": [],
      "source": [
        "## Drawing the lane"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Draw Lane (Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_lane(img,bird_eye,left_fit,right_fit):\n",
        "    tmp_image     = np.copy(img)\n",
        "    if right_fit is None or left_fit is None:\n",
        "        return img\n",
        "    \n",
        "    zero          = np.zeros_like(bird_eye).astype(np.uint8)\n",
        "    layered_image = np.dstack((zero,zero,zero))\n",
        "    \n",
        "    ploty      = np.linspace(0, bird_eye.shape[0]-1, bird_eye.shape[0] )\n",
        "\n",
        "    left_fitx  = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
        "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2] \n",
        "    \n",
        "    #formatting the points\n",
        "    left   = np.array([np.transpose(np.vstack([right_fitx,ploty]))])\n",
        "    right  = np.array([np.flipud(np.transpose(np.vstack([left_fitx,ploty])))])\n",
        "    points = np.hstack((left,right))\n",
        "    # print(np.int_([points]))\n",
        "    \n",
        "    #form lane\n",
        "    cv2.fillPoly(layered_image,np.int32([points]),(0,255,0))\n",
        "    cv2.polylines(layered_image,np.int32([right]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    cv2.polylines(layered_image,np.int32([left]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    \n",
        "    # The inverse perspective transfom note\n",
        "    # use the inverse perspective option mentioned in the note above to transform back the layered_image\n",
        "    inversed = perspective_transform(layered_image, inverse=True)\n",
        "    \n",
        "    output   = cv2.addWeighted(tmp_image,1,inversed,0.5,0)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# left_fit_x, left_fit_y, (right_fit_x, right_fit_y = detectlanes(20,200,10,left_peak,right_peak, transformed_image)\n",
        "def draw_lane_updated(img,bird_eye,left_fit,right_fit, verbose=0):\n",
        "    tmp_image = np.copy(img)\n",
        "    zero          = np.zeros_like(bird_eye).astype(np.uint8)\n",
        "    layered_image = bird_eye\n",
        "    x_new = np.linspace(0, bird_eye.shape[0]-1, bird_eye.shape[0] )\n",
        "    # x_new = np.linspace(left_fit_x[0], left_fit_x[-1], num=len(left_fit_x)*10)\n",
        "\n",
        "    # coefs = poly.polyfit(left_fit_y,left_fit_x ,2)\n",
        "    # ffit = poly.polyval(x_new, coefs)\n",
        "    # plt.plot(x_new, ffit)\n",
        "    # # print(left_fit_x[0])\n",
        "    # plt.plot(right_fit_x,right_fit_y)\n",
        "\n",
        "    left_coefs = poly.polyfit(left_fit[:,1],left_fit[:,0] ,2)\n",
        "    right_coefs = poly.polyfit(right_fit[:,1],right_fit[:,0] ,2)\n",
        "\n",
        "    left_fit_values = poly.polyval(x_new, left_coefs)\n",
        "    right_fit_values = poly.polyval(x_new, right_coefs)\n",
        "\n",
        "    \n",
        "    # print(left_fit[:,0])\n",
        "    # print(left_fit[:,1])\n",
        "    # print(right_fit[:,0])\n",
        "    # print(right_fit[:,1])\n",
        "\n",
        "    # print(right_fit_values)\n",
        "    # print(right_fit_values)\n",
        "\n",
        "    # print(left_fit[:,0])\n",
        "    # print(left_fit[:,1])\n",
        "        # visualize \n",
        "    if(verbose > 0):\n",
        "        f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "        ax1.set_title('Left Fit data on transformed image', fontsize=20)\n",
        "        ax1.plot(left_fit_values, x_new)\n",
        "        ax1.plot(left_fit[:,0],left_fit[:,1])\n",
        "        ax1.imshow(bird_eye)\n",
        "        # ax1.imshow(image_in_action)\n",
        "\n",
        "        ax2.set_title('Right Transformed image', fontsize=20)\n",
        "        ax2.plot(right_fit_values, x_new)\n",
        "        ax2.plot(right_fit[:,0],right_fit[:,1])\n",
        "        ax2.imshow(bird_eye)\n",
        "\n",
        "    zero = np.zeros_like(bird_eye).astype(np.uint8)\n",
        "\n",
        "\n",
        "    arr = np.array((left_fit_values,x_new))\n",
        "    left = np.reshape(np.ravel(arr),(-1,2),order='F')\n",
        "    arr = np.array((right_fit_values,x_new))\n",
        "    right = np.reshape(np.ravel(arr),(-1,2),order='F')\n",
        "    points = np.append(left,np.flip(right,axis=0),axis=0)\n",
        "\n",
        "    # print(np.int32(points))\n",
        "    cv2.polylines(zero,np.int32([left]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    cv2.polylines(zero,np.int32([right]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    # cv2.polylines(bird_eye_view,np.int32([points]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    cv2.fillPoly(zero,np.int32([points]),(0,255,0))\n",
        "    inversed = perspective_transform(zero, inverse=True)    \n",
        "    # plt.imshow(tmp_image)\n",
        "    # plt.imshow(img)\n",
        "\n",
        "\n",
        "\n",
        "        # ax2.imshow(image_in_action)\n",
        "    output = cv2.addWeighted(tmp_image,1,inversed,0.5,0)\n",
        "    return output\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_final_image(input_image, verbose=0):\n",
        "    sub_verbose = verbose - 1\n",
        "    calibrated_image = undistort_image(input_image, sub_verbose - 1)\n",
        "    bird_eye_view = perspective_transform(calibrated_image, verbose=sub_verbose - 1)\n",
        "    binary_img = get_binary_thresholded_img(bird_eye_view,verbose=sub_verbose,yellow_thresh=(150,200),white_thresh=(220,255))\n",
        "    # cv2.imshow(\"Test.jpg\",calibrated_image)\n",
        "    # cv2.waitKey(0) \n",
        "    \n",
        "    left_fit, right_fit = detectlanes(7,200,4,binary_img, verbose=sub_verbose)\n",
        "    output_img = draw_lane_updated(calibrated_image,binary_img,left_fit, right_fit, verbose=sub_verbose)\n",
        "    np_output_img = np.asarray(output_img)\n",
        "\n",
        "    f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "    ax1.set_title('Original image', fontsize=20)\n",
        "    ax1.imshow(calibrated_image)\n",
        "    ax2.set_title('Detected image', fontsize=20)\n",
        "    ax2.imshow(np_output_img)\n",
        "    return output_img\n",
        "# cv2.imshow(\"Test.jpg\",output_img)\n",
        "# cv2.waitKey(0) "
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "P-lHI2bCJawO"
      },
      "outputs": [],
      "source": [
        "## Determine The Lane Curvature\n",
        "You're getting very close to a final result! You have a thresholded image, where you've estimated which pixels belong to the left and right lane lines, and you've fit a polynomial to those pixel positions. Next we'll compute the radius of curvature of the fit.\n",
        "\n",
        "## Curvature in Pixels\n",
        "In the last step we computed the lane line pixels using their x and y pixel positions to fit a second order polynomial curve: $$f(y) = Ay^2+By+C $$\n",
        "in this step you will compute the radius of curvature at the closest point to the vehicle.\n",
        "\n",
        "**Radius of Curvature Equation:**\n",
        "$$R\\_Curve = \\frac{[1+(\\frac{dx}{dy})^2]^{3/2}}{|\\frac{d^2x}{dy^2}|}$$\n",
        "\n",
        "$$f'(y) = \\frac{dx}{dy} = 2Ay+B$$\n",
        "\n",
        "$$f''(y) = \\frac{d^2x}{dy^2} =A$$\n",
        "\n",
        "## From Pixels to Real World\n",
        "* Great! You've now calculated the radius of curvature for our lane lines. But now we need to stop and think... We've calculated the radius of curvature based on pixel values, so the radius we are reporting is in pixel space, which is not the same as real world space. So we actually need to repeat this calculation after converting our x and y values to real world space.\n",
        "\n",
        "* This involves measuring how long and wide the section of lane is that we're projecting in our warped image. We could do this in detail by measuring out the physical lane in the field of view of the camera, but for this project, you can assume that if you're projecting a section of lane similar to the images above, the lane is about 30 meters long and 3.7 meters wide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_r_curve(active_pixels):\n",
        "    active_pixels = active_pixels.reshape(active_pixels.shape[1], active_pixels.shape[0])\n",
        "\n",
        "    x = active_pixels[0]\n",
        "    y = active_pixels[1]\n",
        "    p = np.polyfit(x, y, 2)\n",
        "    a, b, c = p\n",
        "\n",
        "    dx_dy_coff = np.polyder(p)\n",
        "    dx_dy_2_coff = np.polyder(p, m=2)\n",
        "\n",
        "    print(\"JUST TEST\")\n",
        "    print(dx_dy_coff)\n",
        "    print(dx_dy_2_coff)\n",
        "\n",
        "    dx_dy = 2*a*y + b\n",
        "    dx_dy_2 = a\n",
        "\n",
        "    numerator = (1 + dx_dy**2)**(3/2)\n",
        "    denominator = np.abs(dx_dy_2)\n",
        "    r_curve = numerator/denominator\n",
        "    return r_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Image "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the final image \n",
        "get_final_image(sample_image, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_images(path):\n",
        "    import glob\n",
        "\n",
        "    #read in and make a list of calibration images\n",
        "    images = glob.glob(path)\n",
        "    output_images = []\n",
        "    for fname in images:\n",
        "        #read in each image\n",
        "        img = mpimg.imread(fname)   \n",
        "        output_images.append(img)\n",
        "\n",
        "    return output_images"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply on multiple images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_images = load_images('./test_images/*.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for img in all_images:\n",
        "    get_final_image(img, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# left_r_curve = calc_r_curve(left_active_pixels)\n",
        "# right_r_curve = calc_r_curve(right_active_pixels)\n",
        "\n",
        "# left_fit_xy = left_fit.reshape(left_fit.shape[1], left_fit.shape[0])\n",
        "# right_fit_xy = right_fit.reshape(right_fit.shape[1], right_fit.shape[0])\n",
        "# print(right_fit_xy[1][0])\n",
        "# right_fit_xy = np.flip(right_fit_xy, axis=1)\n",
        "\n",
        "\n",
        "# print(right_fit_xy[1][right_fit_xy.shape[1]-1])\n",
        "# f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "# ax1.set_title('left lane original', fontsize=20)\n",
        "# ax1.plot(left_fit_xy[0],left_fit_xy[1])\n",
        "# ax2.set_title('right lane original', fontsize=20)\n",
        "# ax2.plot(right_fit_xy[0],right_fit_xy[1])\n",
        "# print(left_fit_xy.shape)\n",
        "\n",
        "# # # NOT SURE YET\n",
        "# # left_fit = left_r_curve\n",
        "# # right_fit = right_r_curve\n",
        "# left_xs = [left_p1[0], left_p2[0]]\n",
        "# left_ys = [left_p1[1], left_p2[1]]\n",
        "# right_xs = [right_p1[0], right_p2[0]]\n",
        "# right_ys = [right_p1[1], right_p2[1]]\n",
        "# print(\"points\")\n",
        "# print(left_xs)\n",
        "# print(left_ys)\n",
        "# left_fit_p = np.polyfit(left_fit[0], left_fit[1], 2)\n",
        "# right_fit_p = np.polyfit(right_fit[0], right_fit[1], 2)\n",
        "# print(\"polynomials\")\n",
        "# print(left_fit_p)\n",
        "# print(right_fit_p)\n",
        "# bird_eye_view = perspective_transform(calibrated_image)\n",
        "# defined_lane_image = None\n",
        "\n",
        "# defined_lane_image = draw_lane(calibrated_image, bird_eye_view, left_fit_p, right_fit_p)\n",
        "# # print(defined_lane_image)\n",
        "# # print(defined_lane_image.shape)\n",
        "\n",
        "\n",
        "# layered_image = np.zeros_like(bird_eye_view)\n",
        "# left_p1 = np.array([left_fit_xy[0][0], left_fit_xy[1][0]])\n",
        "# left_p2 = np.array([left_fit_xy[0][left_fit_xy.shape[1]-1], left_fit_xy[1][left_fit_xy.shape[1]-1]])\n",
        "# right_p1 = np.array([right_fit_xy[0][0], right_fit_xy[1][0]])\n",
        "# right_p2 = np.array([right_fit_xy[0][right_fit_xy.shape[1]-1], right_fit_xy[1][right_fit_xy.shape[1]-1]])\n",
        "# left = [left_p1, left_p2]\n",
        "# right = [right_p1, right_p2]\n",
        "# points = np.array([left_p1, left_p2, right_p1, right_p2])\n",
        "# print(points)\n",
        "# cv2.fillPoly(layered_image, np.int32([points]), (0,255,0))\n",
        "# cv2.polylines(layered_image,np.int32([right]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "# cv2.polylines(layered_image,np.int32([left]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "\n",
        "\n",
        "\n",
        "# # The inverse perspective transfom note\n",
        "# # use the inverse perspective option mentioned in the note above to transform back the layered_image\n",
        "# inversed = perspective_transform(layered_image, inverse=True)\n",
        "\n",
        "# tmp_image     = np.copy(calibrated_image)\n",
        "# output   = cv2.addWeighted(tmp_image,1,inversed,0.5,0)\n",
        "\n",
        "\n",
        "# f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "# ax1.set_title('manual lines-in-prespective', fontsize=20)\n",
        "# ax1.imshow(layered_image)\n",
        "# ax2.set_title('auto LANE Image transform', fontsize=20)\n",
        "# ax2.imshow(perspective_transform(defined_lane_image))\n",
        "\n",
        "# f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "# ax1.set_title('MANUAL DETECTED LANE Image', fontsize=20)\n",
        "# ax1.imshow(output)\n",
        "# ax2.set_title('POLY DETECTED LANE Image', fontsize=20)\n",
        "# ax2.imshow(defined_lane_image)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}