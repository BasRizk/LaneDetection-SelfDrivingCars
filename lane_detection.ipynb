{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.5.2 64-bit ('carnd-term2': conda)",
      "language": "python",
      "name": "python35264bitcarndterm2conda9eb47d4662e840258ebdf8299956c005"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2-final"
    },
    "colab": {
      "name": "Project_1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "82uA9wzmJav4"
      },
      "outputs": [],
      "source": [
        "# Project Steps\n",
        " ## Main Steps:\n",
        "\n",
        "* Camera calibration and distortion correction.\n",
        "* Color/gradient threshold.\n",
        "* Perspective transform.\n",
        "* Detect lane lines.\n",
        "\n",
        "## Extra Step:\n",
        "* Determine the lane curvature."
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "CzLFGoFcdN2-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "test_dir = \"test_images\"\n",
        "test_files = os.listdir(test_dir)\n",
        "test_files"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "zYIX_WJ5Jav5"
      },
      "outputs": [],
      "source": [
        "# Camera Calibration  and Distortion Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "KAVcKmXveh6_"
      },
      "outputs": [],
      "source": [
        "\n",
        "*   Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cameraCalibrate(image):\n",
        "    # Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
        "    import glob\n",
        "\n",
        "    #read in and make a list of calibration images\n",
        "    images = glob.glob('./camera_cal/calibration*.jpg')\n",
        "\n",
        "    #Arrays to store objects points and image points from all the images\n",
        "    objpoints = [] #3d points\n",
        "    imgpoints = [] #2d points\n",
        "\n",
        "    #prepare points\n",
        "    objp  = np.zeros((9*6,3),np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
        "\n",
        "\n",
        "    for fname in images:\n",
        "        #read in each image\n",
        "        img = mpimg.imread(fname)\n",
        "        #convert image to gray scale  \n",
        "        gray  = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
        "        #find corners  \n",
        "        ret,corners = cv2.findChessboardCorners(gray,(9,6),None)\n",
        "        if ret == True:\n",
        "            imgpoints.append(corners)\n",
        "            objpoints.append(objp)\n",
        "            \n",
        "    shape =(img.shape[1],img.shape[0])\n",
        "    ret, mtx, dist, rvect, tvect = cv2.calibrateCamera(objpoints,imgpoints,shape,None,None)\n",
        "    \n",
        "    #undistored the image\n",
        "    undistorted_image = cv2.undistort(image, mtx, dist, None, mtx)\n",
        "    return undistorted_image"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "NgpLeFaRe9s2"
      },
      "outputs": [],
      "source": [
        "* Apply (Calibrate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_image = mpimg.imread(test_dir + '/test3.jpg')\n",
        "# sample_image = mpimg.imread('./camera_cal/calibration1.jpg')\n",
        "\n",
        "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax1.set_title('Original image', fontsize=20)\n",
        "ax1.imshow(sample_image)\n",
        "ax2.set_title('calibrated image', fontsize=20)\n",
        "calibrated_image=cameraCalibrate(sample_image)\n",
        "ax2.imshow(cameraCalibrate(sample_image))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "PULmYxJQJav-"
      },
      "outputs": [],
      "source": [
        "# Gradient / Color Threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize(ch1, ch2, ch3,\n",
        "            ch1_name, ch2_name, ch3_name,\n",
        "            plot):\n",
        "    plot[0].set_title(ch1_name + '_channel', fontsize=20)\n",
        "    plot[0].imshow(ch1,cmap='gray')\n",
        "    plot[1].set_title(ch2_name + '_channel', fontsize=20)\n",
        "    plot[1].imshow(ch2,cmap='gray')\n",
        "    plot[2].set_title(ch3_name + '_channel', fontsize=20)\n",
        "    plot[2].imshow(ch3,cmap='gray')\n",
        "\n",
        "def binarize(array, l_thresh, u_thresh, yes=1, no=0):\n",
        "    binary = (array >= l_thresh) * array\n",
        "    binary = (array <= u_thresh) * binary\n",
        "    binary = (binary > 0)*yes\n",
        "    return binary\n",
        "\n",
        "# Color threshold for the white line\n",
        "def white(img, channel_num = 0, white_thresh = (220,255)):\n",
        "    channel = img[:,:,channel_num]    \n",
        "    l_thresh, u_thresh = white_thresh[:]\n",
        "    binary = binarize(channel, l_thresh, u_thresh, yes=255, no=0)\n",
        "    return binary\n",
        "\n",
        "# Color threshold for the yellow line\n",
        "def yellow(img, channel_num = 2, yellow_thresh = (220,255)):\n",
        "    channel = img[:,:,channel_num]\n",
        "    l_thresh, u_thresh = yellow_thresh[:]\n",
        "    # binary = cv2.inRange(img, np.array([220, 220, 220]), np.array([255, 255, 255]))\n",
        "    binary = binarize(channel, l_thresh, u_thresh, yes=255, no=0)\n",
        "    return binary"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gHn67GNZJav_"
      },
      "outputs": [],
      "source": [
        "* Possible Models\n",
        "\n",
        "-- HLS Model\n",
        "\n",
        "-- HSV Model\n",
        "\n",
        "-- LAB Model\n",
        "\n",
        "-- RGB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_in_action = calibrated_image\n",
        "f, plot_axs = plt.subplots(4, 3, figsize=(20,20))\n",
        "\n",
        "hls_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2HLS)\n",
        "h_channel = hls_image[:,:,0]\n",
        "l_channel = hls_image[:,:,1]\n",
        "s_channel = hls_image[:,:,2]\n",
        "visualize(h_channel, l_channel, s_channel, \"h\", \"l\", \"s\", plot_axs[0])\n",
        "\n",
        "hsv_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2HSV)\n",
        "h_channel = hsv_image[:,:,0]\n",
        "s_channel = hsv_image[:,:,1]\n",
        "v_channel = hsv_image[:,:,2]\n",
        "visualize(h_channel, s_channel, v_channel, \"h\", \"s\", \"v\", plot_axs[1])\n",
        "\n",
        "lab_image = cv2.cvtColor(image_in_action,cv2.COLOR_RGB2LAB)\n",
        "l_channel = lab_image[:,:,0]\n",
        "a_channel = lab_image[:,:,1]\n",
        "b_channel = lab_image[:,:,2]\n",
        "visualize(l_channel, a_channel, b_channel, \"l\", \"a\", \"b\", plot_axs[2])\n",
        "\n",
        "rgb_image = image_in_action\n",
        "r_channel = rgb_image[:,:,0]\n",
        "g_channel = rgb_image[:,:,1]\n",
        "b_channel = rgb_image[:,:,2]\n",
        "visualize(r_channel, g_channel, b_channel, \"r\", \"g\", \"b\", plot_axs[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_in_action = calibrated_image\n",
        "binary_white = white(image_in_action, channel_num=0, white_thresh = (245,255))\n",
        "binary_yellow = yellow(lab_image, channel_num=2, yellow_thresh = (170,200))\n",
        "\n",
        "# combine both the white and yellow binaries in combined_binary\n",
        "binary_combined = np.zeros_like(image_in_action)\n",
        "binary_combined[(binary_white >= 1) | (binary_yellow >= 1)] = 255\n",
        "\n",
        "# visualize\n",
        "f, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20,10))\n",
        "ax1.set_title('1st channel', fontsize=20)\n",
        "ax1.imshow(binary_white,cmap='gray')\n",
        "ax2.set_title('2nd channel', fontsize=20)\n",
        "ax2.imshow(binary_yellow,cmap='gray')\n",
        "ax3.set_title('combined', fontsize=20)\n",
        "ax3.imshow(binary_combined,cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_in_action = cv2.merge((binary_combined,binary_combined,binary_combined))\n",
        "# print(np.max(image_in_action))\n",
        "print(calibrated_image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "o4k2Dpf2JawE"
      },
      "outputs": [],
      "source": [
        "# Perspective Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perspective_transform(undistorted, inverse=False):   \n",
        "\n",
        "    \n",
        "    img = undistorted\n",
        "    w = image_in_action.shape[1]\n",
        "    h = image_in_action.shape[0]\n",
        "    img_size = (w, h)\n",
        "    src = np.float32([[w/8,h],[3*w/8,4*h/6],\n",
        "                  [5*w/8,4*h/6],[7*w/8,h]])\n",
        "    # make sure that the points follow the right arrangement whether it's clockwise or counter-clockwise\n",
        "    # source and destination points must have the same arrangement whether it's clockwise or counter-clockwise\n",
        "    # The points in src array are (x,y).\n",
        "    dst = np.float32([[2*w/10,h],[2*w/10,2*h/10],\n",
        "                    [8*w/10,2*h/10],[8*w/10,h]])\n",
        "\n",
        "    if(inverse):\n",
        "        M = cv2.getPerspectiveTransform(dst, src)\n",
        "    else:\n",
        "        M = cv2.getPerspectiveTransform(src, dst)\n",
        "\n",
        "    transformed_img = cv2.warpPerspective(undistorted, M, img_size, flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # gray_scale = False\n",
        "\n",
        "    # if len(img.shape) == 2:\n",
        "    #     print(img.shape)\n",
        "    #     gray_scale = True\n",
        "    #     img = cv2.cvtColor(undistorted, cv2.COLOR_GRAY2RGB)\n",
        "    # M = cv2.getPerspectiveTransform(src, dst)\n",
        "    # transformed_img = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
        "\n",
        "\n",
        "    # if gray_scale:\n",
        "    #     transformed_img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    return transformed_img"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "* Apply (Transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make sure that the points follow the right arrangement whether it's clockwise or counter-clockwise\n",
        "# source and destination points must have the same arrangement whether it's clockwise or counter-clockwise\n",
        "# The points in src array are (x,y).\n",
        "# task_image = cameraCalibrate(task_image)\n",
        "image_in_action = binary_combined\n",
        "# print(image_in_action.shape)\n",
        "w = image_in_action.shape[1]\n",
        "h = image_in_action.shape[0]\n",
        "img_size = (w, h)\n",
        "src = np.float32([[w/8,h],[3*w/8,4*h/6],\n",
        "                [5*w/8,4*h/6],[7*w/8,h]])\n",
        "# make sure that the points follow the right arrangement whether it's clockwise or counter-clockwise\n",
        "# source and destination points must have the same arrangement whether it's clockwise or counter-clockwise\n",
        "# The points in src array are (x,y).\n",
        "dst = np.float32([[2*w/10,h],[2*w/10,2*h/10],\n",
        "                [8*w/10,2*h/10],[8*w/10,h]])\n",
        "\n",
        "# visualize your mask region\n",
        "f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "ax1.set_title('source area', fontsize=20)\n",
        "ax1.imshow(image_in_action)\n",
        "ordered_y = [src[0][1],src[1][1],src[2][1],src[3][1],src[0][1]]\n",
        "ordered_x = [src[0][0],src[1][0],src[2][0],src[3][0],src[0][0]]\n",
        "ax1.plot(ordered_x,ordered_y , color='red', alpha=0.7,\n",
        "    linewidth=3, solid_capstyle='round', zorder=2)\n",
        "\n",
        "# visualize your mask region\n",
        "f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "ax1.set_title('destination area', fontsize=20)\n",
        "ax1.imshow(image_in_action)\n",
        "ordered_y = [dst[0][1],dst[1][1],dst[2][1],dst[3][1],dst[0][1]]\n",
        "ordered_x = [dst[0][0],dst[1][0],dst[2][0],dst[3][0],dst[0][0]]\n",
        "ax1.plot(ordered_x,ordered_y , color='red', alpha=0.7,\n",
        "    linewidth=3, solid_capstyle='round', zorder=2)\n",
        "\n",
        "# Undistord the image then apply perspective transformation\n",
        "transformed_image = perspective_transform(image_in_action)\n",
        "# visualize your results\n",
        "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax1.set_title('Original image', fontsize=20)\n",
        "ax1.imshow(image_in_action)\n",
        "ax2.set_title('Transformed image', fontsize=20)\n",
        "ax2.imshow(transformed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "VzZTfbdgJawH"
      },
      "outputs": [],
      "source": [
        "# Detect Lane Lines\n",
        "\n",
        "After finishing the previous steps You now have a thresholded warped image and you're ready to map out the lane lines! There are many ways you could go about this, but here's one example of how you might do it:\n",
        "### Peaks in a Histogram and Sliding Windows\n",
        "* After applying calibration, thresholding, and a perspective transform to a road image, you should have a binary image where the lane lines stand out clearly. However, you still need to decide explicitly which pixels are part of the lines and which belong to the left line and which belong to the right line.\n",
        "* we can use the two highest peaks from our histogram as a starting point for determining where the lane lines are, and then use sliding windows moving upward in the image (further along the road) to determine where the lane lines go.\n",
        "#### steps:\n",
        "  1. split the histogram into two sides, one for each lane line.\n",
        "  2. Set up sliding windows and window hyperparameters:\n",
        "     * set a few hyperparameters related to our sliding windows, and set them up to iterate across the binary activations in the image. These hyperparameters are:\n",
        "        1. **W_Number**; number of sliding windows.\n",
        "        2. **Margin**; the width of each window.\n",
        "        3. **Minimum_pixels**; used as a threshold to recenter the next sliding window.\n",
        "        4. **Window_Height**; computed from number of pixels and image height.\n",
        "  3. Loop through each window in W_Number.\n",
        "  4. Find the boundaries of our current window. This is based on a combination of the current window's starting point      , as well as the margin you set in the hyperparameters.\n",
        "  5. Use cv2.rectangle to draw these window boundaries onto visualization image.\n",
        "  6. Now that we know the boundaries of our window, find out which activated(non zero) pixels actually fall into the window.\n",
        "  7. Append these non zero pixels to two different arrays one for the right line and the other for the left line.\n",
        "  8. If the number of pixels you found in Step **6** are greater than your hyperparameter Minimum_pixels, re-center our window based on the mean position of these pixels.\n",
        "  9. Now that we have found all our pixels belonging to each line through the sliding window method, it's time to fit a polynomial to the line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_in_action = calibrated_image\n",
        "binary_white = white(image_in_action, channel_num=0, white_thresh = (245,255))\n",
        "def detectlanes(w_number, margin, minimum_pixels, left_peak, right_peak, binary_img):\n",
        "\n",
        "    out_img = binary_img.copy()\n",
        "    # get the image's width and height.\n",
        "    w = out_img.shape[1]\n",
        "    h = out_img.shape[0]\n",
        "\n",
        "    # 2. Set up sliding windows and window hyperparameters:\n",
        "    window_height = int(h // w_number)\n",
        "\n",
        "    half_window_height = int(window_height // 2)\n",
        "    half_window_width = int(margin // 2)\n",
        "    current_height = int(h - half_window_height)\n",
        "\n",
        "    left_window_center = int(left_peak)\n",
        "    right_window_center = int(right_peak)\n",
        "\n",
        "    \n",
        "    \n",
        "    # 3. Loop through each window in W_Number.\n",
        "    for i in range(0, w_number):\n",
        "\n",
        "        left_window_boundaries = [( left_window_center - half_window_width, current_height + half_window_height) , (left_window_center + half_window_width, current_height - half_window_height)]\n",
        "        right_window_boundaries = [(right_window_center - half_window_width, current_height + half_window_height) , (right_window_center + half_window_width, current_height - half_window_height)]\n",
        "        # print(left_window_boundaries[0])\n",
        "        # print(left_window_boundaries[1])\n",
        "        \n",
        "        # print(left_window_boundaries[0])\n",
        "        # left_window = out_img[left_window_boundaries[1][1]: left_window_boundaries[0][1],left_window_boundaries[0][0]:left_window_boundaries[1][0],:]\n",
        "        # right_window = out_img[right_window_boundaries[1][1]: right_window_boundaries[0][1],right_window_boundaries[0][0]:right_window_boundaries[1][0],:]\n",
        "        left_window_indices, left_horizontal_indcies = window_helper(left_window_boundaries[1][1],left_window_boundaries[0][1],left_window_boundaries[0][0],left_window_boundaries[1][0], out_img)\n",
        "\n",
        "\n",
        "        right_window_indices, right_horizontal_indcies = window_helper(right_window_boundaries[1][1],right_window_boundaries[0][1],right_window_boundaries[0][0],right_window_boundaries[1][0], out_img)\n",
        "\n",
        "        # print(current_height)\n",
        "        # print(right_window_center)\n",
        "        if(len(left_horizontal_indcies) > minimum_pixels):\n",
        "            left_window_center = int(np.mean(left_horizontal_indcies))\n",
        "            left_window_boundaries = [( left_window_center - half_window_width, current_height + half_window_height) , (left_window_center + half_window_width, current_height - half_window_height)]\n",
        "\n",
        "        if(len(right_horizontal_indcies) > minimum_pixels):\n",
        "            right_window_center = int(np.mean(right_horizontal_indcies))\n",
        "            right_window_boundaries = [(right_window_center - half_window_width, current_height + half_window_height) , (right_window_center + half_window_width, current_height - half_window_height)]\n",
        "\n",
        "\n",
        "\n",
        "            for index in left_window_indices:\n",
        "                # print(index)\n",
        "                cv2.circle(out_img, tuple(index), 1, (255, 255, 0) , -1) \n",
        "\n",
        "            for index in right_window_indices:\n",
        "                cv2.circle(out_img, tuple(index), 1, (255, 255, 0) , -1)    \n",
        "\n",
        "            cv2.rectangle(out_img, left_window_boundaries[0], left_window_boundaries[1], (0, 255, 0), 2)\n",
        "            cv2.rectangle(out_img, right_window_boundaries[0], right_window_boundaries[1], (0, 255, 0), 2)\n",
        "            cv2.circle(out_img, (left_window_center,current_height), 2, (255, 0, 0) , -1) \n",
        "            cv2.circle(out_img, (right_window_center,current_height), 2, (255, 0, 0) , -1) \n",
        "\n",
        "            if(i == 0):\n",
        "                left_fit = left_window_indices\n",
        "                right_fit = right_window_indices\n",
        "            else:\n",
        "                left_fit = np.append(left_fit, left_window_indices, axis=0)\n",
        "                right_fit = np.append(right_fit, right_window_indices , axis =0)\n",
        "\n",
        "\n",
        "        current_height -= window_height\n",
        "\n",
        "    f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "    ax1.set_title('Transformed Image', fontsize=20)\n",
        "\n",
        "    ax1.imshow(out_img)\n",
        "    # print(left_fit.shape)\n",
        "    return left_fit, right_fit\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def window_helper(min_height, max_height, min_width, max_width, img):\n",
        "    test = img[min_height:max_height,min_width:max_width,:]\n",
        "    # print(img_copy.shape)\n",
        "    # print(test.shape)\n",
        "    x,y,z = np.where(test == (255,255,255))\n",
        "    x += min_height\n",
        "    y += min_width\n",
        "    # print(x)\n",
        "    # print(y)\n",
        "    # mean = 0\n",
        "    # if(len(x) > min_pixels):\n",
        "    #     mean = (int(np.mean(x)), int(np.mean(y)))\n",
        "    arr = np.array((y,x))\n",
        "\n",
        "    # print(arr[0])\n",
        "    # print(arr[1])\n",
        "    final = np.reshape(np.ravel(arr),(-1,2),order='F')\n",
        "    return final, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "binary_white = white(calibrated_image, channel_num=0, white_thresh = (245,255))\n",
        "binary_yellow = yellow(lab_image, channel_num=2, yellow_thresh = (170,200))\n",
        "\n",
        "# combine both the white and yellow binaries in combined_binary\n",
        "binary_combined = np.zeros_like(image_in_action)\n",
        "binary_combined[(binary_white >= 1) | (binary_yellow >= 1)] = 255\n",
        "transformed_image = perspective_transform(binary_combined)\n",
        "\n",
        "image_histo = np.zeros_like(transformed_image)\n",
        "image_histo[transformed_image >= 1] = 1 \n",
        "image_histo = image_histo[:,:,1]\n",
        "image_histo = image_histo.reshape(image_histo.shape[1], image_histo.shape[0])\n",
        "image_histo = image_histo.sum(axis=1)\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax1.set_title('Transformed Image', fontsize=20)\n",
        "ax1.imshow(transformed_image)\n",
        "ax2.set_title('Histogram', fontsize=20)\n",
        "x_values = np.array([i for i in range(0, transformed_image.shape[1])])\n",
        "ax2.plot(x_values, image_histo)\n",
        "\n",
        "image_center = int(image_histo.shape[0]/2)\n",
        "left_peak = np.argmax(image_histo[:image_center])\n",
        "right_peak = np.argmax(image_histo[image_center:]) + image_center\n",
        "print(\"Left peak %s, Right peak %s\" % (left_peak, right_peak))\n",
        "\n",
        "detectlanes(8,100,10,left_peak,right_peak, transformed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "NaUKAAhqJawL"
      },
      "outputs": [],
      "source": [
        "## This function is to draw back the lane on the original image\n",
        "**N.B.** Add an inverse option in the perspective function so that we can transform the image back to it's original perspective. This can be implemented by swapping **src** and **dst** in cv2.getPerspectiveTransform(src, dst) when a certain flag is set to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#draw lane\n",
        "# left_fit and right_fit generated from step 9 in lane Detection\n",
        "# bird_eye the image after thresholding and perspective transform\n",
        "# read the inverse note below in the code\n",
        "def draw_lane(img,bird_eye,left_fit,right_fit):\n",
        "    tmp_image     = np.copy(img)\n",
        "    if right_fit is None or left_fit is None:\n",
        "        return img\n",
        "    \n",
        "    zero          = np.zeros_like(bird_eye).astype(np.uint8)\n",
        "    layered_image = np.dstack((zero,zero,zero))\n",
        "    \n",
        "    ploty      = np.linspace(0, bird_eye.shape[0]-1, bird_eye.shape[0] )\n",
        "    left_fitx  = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
        "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2] \n",
        "    \n",
        "    #formatting the points\n",
        "    left   = np.array([np.transpose(np.vstack([right_fitx,ploty]))])\n",
        "    right  = np.array([np.flipud(np.transpose(np.vstack([left_fitx,ploty])))])\n",
        "    points = np.hstack((left,right))\n",
        "    \n",
        "    #form lane\n",
        "    cv2.fillPoly(layered_image,np.int_([points]),(0,255,0))\n",
        "    cv2.polylines(layered_image,np.int32([right]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    cv2.polylines(layered_image,np.int32([left]),isClosed = False,color=(255,0,0),thickness = 20)\n",
        "    \n",
        "    # The inverse perspective transfom note\n",
        "    # use the inverse perspective option mentioned in the note above to transform back the layered_image\n",
        "    inversed = perspective_transform(layered_image, inverse=True)\n",
        "    \n",
        "    output   = cv2.addWeighted(tmp_image,1,inversed,0.5,0)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "P-lHI2bCJawO"
      },
      "outputs": [],
      "source": [
        "## Determine The Lane Curvature\n",
        "You're getting very close to a final result! You have a thresholded image, where you've estimated which pixels belong to the left and right lane lines, and you've fit a polynomial to those pixel positions. Next we'll compute the radius of curvature of the fit.\n",
        "\n",
        "## Curvature in Pixels\n",
        "In the last step we computed the lane line pixels using their x and y pixel positions to fit a second order polynomial curve: $$f(y) = Ay^2+By+C $$\n",
        "in this step you will compute the radius of curvature at the closest point to the vehicle.\n",
        "\n",
        "**Radius of Curvature Equation:**\n",
        "$$R\\_Curve = \\frac{[1+(\\frac{dx}{dy})^2]^{3/2}}{|\\frac{d^2x}{dy^2}|}$$\n",
        "\n",
        "$$f'(y) = \\frac{dx}{dy} = 2Ay+B$$\n",
        "\n",
        "$$f''(y) = \\frac{d^2x}{dy^2} =A$$\n",
        "\n",
        "## From Pixels to Real World\n",
        "* Great! You've now calculated the radius of curvature for our lane lines. But now we need to stop and think... We've calculated the radius of curvature based on pixel values, so the radius we are reporting is in pixel space, which is not the same as real world space. So we actually need to repeat this calculation after converting our x and y values to real world space.\n",
        "\n",
        "* This involves measuring how long and wide the section of lane is that we're projecting in our warped image. We could do this in detail by measuring out the physical lane in the field of view of the camera, but for this project, you can assume that if you're projecting a section of lane similar to the images above, the lane is about 30 meters long and 3.7 meters wide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_r_curve(active_pixels):\n",
        "    x = active_pixels[:,0]\n",
        "    y = active_pixels[:,0]\n",
        "    p = np.polyfit(x, y, 2)\n",
        "    a, b, c = p\n",
        "\n",
        "    dx_dy_coff = np.polyder(p)\n",
        "    dx_dy_2_coff = np.polyder(p, m=2)\n",
        "\n",
        "    print(\"JUST TEST\")\n",
        "    print(dx_dy_coff)\n",
        "    print(dx_dy_2_coff)\n",
        "\n",
        "    dx_dy = 2*a*y + b\n",
        "    dx_dy_2 = a\n",
        "\n",
        "    numerator = (1 + dx_dy**2)**(3/2)\n",
        "    denominator = np.abs(dx_dy_2)\n",
        "    r_curve = numerator/denominator\n",
        "    return r_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "left_r_curve = calc_r_curve(left_active_pixels)\n",
        "right_r_curve = calc_r_curve(right_active_pixels)\n",
        "# NOT SURE YET\n",
        "left_fit = left_r_curve\n",
        "right_fit = right_r_curve\n",
        "bird_eye_view = perspective_transform(calibrated_image)\n",
        "defined_lane_image = draw_lane(calibrated_image, left_fit, right_fit)\n",
        "\n",
        "f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "ax2.set_title('Lane Defined Output', fontsize=20)\n",
        "ax2.imshow(defined_lane_image)"
      ]
    }
  ]
}